{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Convolutional Neural Networks\n======\n\nConvolutional neural networks (CNNs) are a class of deep neural networks, most commonly used in computer vision applications.\n\nConvolutional refers the network pre-processing data for you - traditionally this pre-processing was performed by data scientists. The neural network can learn how to do pre-processing *itself* by applying filters for things such as edge detection."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Step 1\n-----\n\nIn this exercise we will train a CNN to recognise handwritten digits, using the MNIST digit dataset.\n\nThis is a very common exercise and data set to learn from.\n\nLet's start by loading our dataset and setting up our train, validation, and test sets."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Run this!\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport tensorflow as tf\nimport numpy as np\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, Dense, Dropout, Flatten, MaxPooling2D\nprint('keras using %s backend'%keras.backend.backend())\nimport matplotlib.pyplot as graph\n%matplotlib inline\ngraph.rcParams['figure.figsize'] = (15,5)\ngraph.rcParams[\"font.family\"] = 'DejaVu Sans'\ngraph.rcParams[\"font.size\"] = '12'\ngraph.rcParams['image.cmap'] = 'rainbow'",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Here we import the dataset, and split it into the training, validation, and test sets.\nfrom keras.datasets import mnist\n\n# This is our training data, with 6400 samples.\n###--- REPLACE THE ???s BELOW WITH train_X AND THEN train_Y ---###\n??? = mnist.load_data()[0][0][:6400].astype('float32')\n??? = mnist.load_data()[0][1][:6400]\n###\n\n# This is our validation data, with 1600 samples.\n###--- REPLACE THE ???s BELOW WITH valid_X AND THEN valid_Y ---###\n??? = mnist.load_data()[1][0][:1600].astype('float32')\n??? = mnist.load_data()[1][1][:1600]\n###\n\n# This is our test data, with 2000 samples.\n###--- REPLACE THE ???s BELOW WITH test_X AND THEN test_Y ---###\n??? = mnist.load_data()[1][0][-2000:].astype('float32')\n??? = mnist.load_data()[1][1][-2000:]\n###\n\nprint('train_X:', train_X.shape, end = '')\nprint(', train_Y:', train_Y.shape)\nprint('valid_X:', valid_X.shape, end = '')\nprint(', valid_Y:', valid_Y.shape)\nprint('test_X:', test_X.shape, end = '')\nprint(', test_Y:', test_Y.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "So we have 6400 training samples, 1600 validation samples, and 2000 test samples.\n\nEach sample is an greyscale image - 28 pixels wide and 28 pixels high. Each pixel is really a number from 0 to 255 - 0 being fully black, 255 being fully white. When we graph the 28x28 numbers, we can see the image.\n\nLet's have a look at one of our samples."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "###--- REPLACE THE ??? BELOW WITH train_X[0] OR ANOTHER SAMPLE ---###\ngraph.imshow(???, cmap = 'gray', interpolation = 'nearest')\n###\n\ngraph.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Step 2\n---\n\nThe neural network will use the 28x28 values of each image to predict what each image represents.\n\nWe need to reshape our data to get it working well with our neural network. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# First off, let's reshape our X sets so that they fit the convolutional layers.\n\n# Image dimensions - 28\ndim = train_X[0].shape[0]\n\n###--- REPLACE THE ???s BELOW WITH reshape ---###\ntrain_X = train_X.???(train_X.shape[0], dim, dim, 1)\nvalid_X = valid_X.???(valid_X.shape[0], dim, dim, 1)\ntest_X = test_X.???(test_X.shape[0], dim, dim, 1)\n###\n\n# Next up - feature scaling.\n# We scale the values so they are between 0 and 1, instead of 0 and 255.\n\n###--- REPLACE THE ???s BELOW WITH /255 ---###\ntrain_X = train_X ???\nvalid_X = valid_X ???\ntest_X = test_X ???\n###\n\n\n# Now we print the label for the first example\nprint(train_Y[0])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Expected output:  \n`5`\n\nThe label is a number - the number we see when we view the image.\n\nWe need represent this number as a one-hot vector, so the neural network knows it is a category.\n\nKeras can convert these labels into one-hot vectors easily with the function - `to_categorical`"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "###--- REPLACE THE ???s BELOW WITH to_categorical ---###\ntrain_Y = keras.utils.???(train_Y, 10)\nvalid_Y = keras.utils.???(valid_Y, 10)\ntest_Y = keras.utils.???(test_Y, 10)\n###\n\n# 10 being the number of categories (numbers 0 to 9)\n\nprint(train_Y[0])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Expected output:  \n`[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]`\n\nStep 3\n-----\n\nAll ready! Time to build another neural network."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Sets a randomisation seed for replicatability.\nnp.random.seed(6)\n\n###--- REPLACE THE ??? BELOW WITH Sequential() (don't forget the () ) ---###\nmodel = ???\n###",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The __Convolutional__ in Convolutional Neural Networks refers the pre-processing the network can do itself."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "###--- REPLACE THE ???s BELOW WITH Conv2D ---###\nmodel.add(???(28, kernel_size = (3, 3), activation = 'relu', input_shape = (dim, dim, 1)))\nmodel.add(???(56, (3, 3), activation = 'relu'))\n###",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next up we'll:\n* Add pooling layers.\n* Apply dropout.\n* Flatten the data to a vector (the output of step 2 is a vector)."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Pooling layers help speed up training time and make features it detects more robust.\n# They act by downsampling the data - reducing the data size and complexity.\n\n###--- REPLACE THE ??? BELOW WITH MaxPooling2D ---###\nmodel.add(???(pool_size = (2, 2)))\n###\n\n# Dropout is a technique to help prevent overfitting\n# It makes nodes 'dropout' - turning them off randomly.\n\n###--- REPLACE THE ??? BELOW WITH Dropout ---###\nmodel.add(???(0.125))\n###\n\n#\n\n###--- REPLACE THE ??? BELOW WITH Flatten ---###\nmodel.add(???())\n###",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Dense layers perform classification - we have extracted the features with the convolutional pre-processing\nmodel.add(Dense(128, activation='relu'))\n\n# More dropout!\nmodel.add(Dropout(0.25))\n\n# Next is our output layer\n# Softmax outputs the probability for each category\n###--- REPLACE ??? BELOW WITH 10, THE NUMBER OF CLASSES (DIGITS 0 TO 9) ---###\nmodel.add(Dense(???, activation=tf.nn.softmax))\n###\n\n# And finally, we compile.\nmodel.compile(loss='categorical_crossentropy', optimizer='Adamax', metrics=['accuracy'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Step 4\n-----\n\nLet's train it!"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "###--- REPLACE THE ???s BELOW WITH train_X, train_Y, valid_X, AND THEN valid_Y ---###\ntraining_stats = model.fit(???, ???, batch_size = 128, epochs = 12, verbose = 1, validation_data = (???, ???))\n###\n\n###--- REPLACE THE ??? BELOW WITH evaluate ---###\nevaluation = model.???(test_X, test_Y, verbose=0)\n###\n\nprint('Test Set Evaluation: loss = %0.6f, accuracy = %0.2f' %(evaluation[0], 100 * evaluation[1]))\n\n# We can plot our training statistics to see how it developed over time\naccuracy, = graph.plot(training_stats.history['acc'], label = 'Accuracy')\ntraining_loss, = graph.plot(training_stats.history['loss'], label = 'Training Loss')\ngraph.legend(handles = [accuracy, training_loss])\nloss = np.array(training_stats.history['loss'])\nxp = np.linspace(0,loss.shape[0],10 * loss.shape[0])\ngraph.plot(xp, np.full(xp.shape, 1), c = 'k', linestyle = ':', alpha = 0.5)\ngraph.plot(xp, np.full(xp.shape, 0), c = 'k', linestyle = ':', alpha = 0.5)\ngraph.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Step 5\n\nLet's test it on a new sample that it hasn't seen, and see how it classifies it!"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "###--- REPLACE THE ??? WITH ANY NUMBER BETWEEN 0 AND 1999 ---###\nsample = test_X[???].reshape(dim, dim)\n###\n\ngraph.imshow(sample, cmap = 'gray', interpolation = 'nearest')\ngraph.show()\n\nprediction = model.predict(sample.reshape(1, dim, dim, 1))\nprint('prediction: %i' %(np.argmax(prediction)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "How is the prediction? Does it look right?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Conclusion\n------\n\nCongratulations! We've built a convolutional neural network that is able to recognise handwritten digits with very high accuracy.\n\nCNN's are very complex - you're not expected to understand everything (or most things) we covered here. They take a lot of time and practise to properly understand each aspect of them.\n\nHere we used:  \n* __Feature scaling__ - reducing the range of the values. This helps improve training time.\n* __Convolutional layers__ - network layers that pre-process the data for us. These apply filters to extract features for the neural network to analyze.\n* __Pooling layers__ - part of the Convolutional layers. They apply filters downsample the data - extracting features.\n* __Dropout__ - a regularization technique to help prevent overfitting.\n* __Dense layers__ - neural network layers which perform classification on the features extracted by the convolutional layers and downsampled by the pooling layers.\n* __Softmax__ - an activation function which outputs the probability for each category."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}