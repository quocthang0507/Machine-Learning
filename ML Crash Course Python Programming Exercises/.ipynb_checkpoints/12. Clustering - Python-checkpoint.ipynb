{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Clustering\n======\n\nWhen a data set doesnâ€™t have labels we can use unsupervised learning to find some kind of structure in the data - allowing us to discover patterns or groupings.\n\nCluster analysis is a method of finding groupings, known as clusters, in datasets. As the data sets are unlabelled, cluster analysis tries to group similar examples using the examples features.\n\nK-means clustering lives true to its name - it separates examples into k number of clusters (so if k is 5, it will divide the examples into 5 clusters) and it partitions the examples by the average (mean) of the clusters."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Step 1\n-----\n\nIn this exercise we will look at using k-means clustering to categorise a few different datasets.\n\nLet's start by first creating three clusters."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This sets up the graphs\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as graph\n%matplotlib inline\ngraph.rcParams['figure.figsize'] = (15,5)\ngraph.rcParams[\"font.family\"] = 'DejaVu Sans'\ngraph.rcParams[\"font.size\"] = '12'\ngraph.rcParams['image.cmap'] = 'rainbow'",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Let's make some data!\nimport numpy as np\nfrom sklearn import datasets\n\n###--- REPLACE THE ??? BELOW WITH cluster_data AND THEN output ---###\n???, ??? = datasets.make_classification(n_samples = 500, n_features = 2, n_informative = 2, n_redundant = 0, n_repeated = 0,\n                                                    n_classes = 3, n_clusters_per_class = 1, class_sep = 1.25, random_state = 6)\n###\n\n# Let's visualise it\ngraph.scatter(cluster_data.T[0], cluster_data.T[1])\ngraph.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now let's see how k-means performs on a dataset like this!"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.cluster import KMeans\n\n###--- REPLACE THE ??? BELOW WITH KMeans ---###\nk_means = ???(n_clusters=3)\n###\n\n###--- REPLACE THE ??? BELOW WITH fit ---###\nk_means.???(cluster_data)\n###\n\n# Let's visualise it\n###--- REPLACE THE ??? BELOW WITH k_means.cluster_centers_ ---###\nfor mean in ???_:\n    graph.plot(mean[0], mean[1], 'ko', marker = '+', markersize = 20)\n###\n\n###--- REPLACE THE ??? BELOW WITH k_means.labels_ ---###\ngraph.scatter(cluster_data.T[0], cluster_data.T[1], c = ???)\n###\n\ngraph.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "It performs rather well, by the looks of it! But we already knew that it had three clusters, sometimes it might not be so clear. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Step 2\n\nLet's generate another dataset in which it may be a little less obvious how many classes it contains."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "###--- REPLACE THE ??? BELOW WITH datasets.make_classification ---###\ncluster_data, output = ???(n_samples = 500, n_features = 2, n_informative = 2, n_redundant = 0, n_repeated = 0, \n                                            n_classes = 4, n_clusters_per_class = 1, class_sep = 1.25, random_state = 6)\n###\n\ngraph.scatter(cluster_data.T[0], cluster_data.T[1])\ngraph.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In instances where we do not know how many classes to expect, it is handy to run k-means multiple times and compare how the data looks when divided up into a differing number of classes. Let's try that now."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "###--- REPLACE THE ???s BELOW WITH n ---###\nfor ??? in range(2,6):\n    k_means = KMeans(n_clusters = ???).fit(cluster_data)\n###\n\n    for mean in k_means.cluster_centers_:\n        graph.plot(mean[0], mean[1], 'ko', marker = '+', markersize = 20)\n    graph.scatter(cluster_data.T[0], cluster_data.T[1], c = k_means.labels_)\n    graph.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Which one do you think best splits the data?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Step 3\n\nK-means clustering performs well enough on clustered data like that, but let's try it out on a dataset that is not so linear."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "###--- REPLACE THE ??? BELOW WITH make_circles ---###\nring_data, target = datasets.???(n_samples = 500, factor = .5, noise = 0.05, random_state = 6)\n###\n\ngraph.scatter(ring_data.T[0], ring_data.T[1], c = target)\ngraph.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We can clearly distinguish two \"clusters\", that is, the two rings of datapoints.\n\nLet's see how k-means handles a dataset like this."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "###--- REPLACE THE ??? BELOW WITH ring_data ---###\nk_means = KMeans(n_clusters = 2).fit(???)\n###\n\nfor mean in k_means.cluster_centers_:\n    graph.plot(mean[0], mean[1], 'ko', marker = '+', markersize = 20)\ngraph.scatter(ring_data.T[0], ring_data.T[1], c = k_means.labels_)\ngraph.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "K-means clearly has difficulty solving this.\n\nAs we are using it, there is no way for k-means to place two means to label this data set correctly."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Step 4\n------\n\nBut, we can try another way. We can use another feature - distance away from the centre.\n\nLet's see if k-means is able to classify the two data clusters with this new feature."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "distance_from_center = []\nfor sample in ring_data:\n###--- REPLACE THE ??? BELOW WITH np.sqrt ---###\n    z = 4 * ???(sample[0]**2 + sample[1]**2)\n###\n    distance_from_center.append(z)\n# Make it a three-dimensional dataset\nring_data = np.concatenate((ring_data, np.array(distance_from_center).reshape(-1, 1)), axis = 1)\n\ngraph.scatter(ring_data.T[0], ring_data.T[1], c = ring_data.T[2])\ngraph.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Looks like it will work, so let's plot all three features."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from mpl_toolkits.mplot3d import Axes3D\n\nfig = graph.figure()\n###--- REPLACE THE ??? BELOW WITH projection='3d' ---###\nax = fig.add_subplot(111, ???)\n###\n\n###--- REPLACE THE ??? BELOW WITH ring_data.T[2] ---###\nax.scatter(ring_data.T[0], ring_data.T[1], ???, c = target)\n###\n\nax.view_init(30, 45)\ngraph.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's see how k-means deals with the data now that it has 3 features!"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "###--- REPLACE THE ??? BELOW WITH ring_data ---###\nk_means = KMeans(n_clusters = 2, random_state = 0).fit(???)\n###\n\nfig = graph.figure()\nax = fig.add_subplot(111, projection='3d')\nfor mean in k_means.cluster_centers_:\n    ax.scatter(mean[0], mean[1], mean[2], c='black', marker='+', s=50) # plot the cluster centres\n    \n###--- REPLACE THE ??? BELOW WITH k_means.labels_ ---###\nax.scatter(ring_data.T[0], ring_data.T[1], ring_data.T[2], c = ???)\n###\n\n# We can plot a hyperplane that separates the two rings\nhp_X, hp_Y = np.array(np.meshgrid(np.linspace(-1, 1, 11), np.linspace(-1, 1, 11)))\nhp_Z = np.full(hp_X.shape, np.abs(k_means.cluster_centers_[0][2] - k_means.cluster_centers_[1][2] / 2))\nax.plot_wireframe(hp_X, hp_Y, hp_Z, rstride = 1, cstride = 1, \n                  color = 'k', linewidth = 1, linestyle = 'solid', alpha = 0.5)\n\nax.view_init(20, 45)\nax.set_zlabel('new axis')\ngraph.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can see the `+` that indicates the center of the clusters. Looks good!\n\nStep 5\n------\n\nSome data we cannot manipulate like that. Let's have a look at a different type of data distribution."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "###--- REPLACE THE ??? BELOW WITH datasets.make_moons ---###\ncrescent_data, output = ???(n_samples = 500, noise = .05)\n###\n\ngraph.scatter(crescent_data.T[0], crescent_data.T[1], c = target)\ngraph.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's try fitting it."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "###--- RUN KMeans ON THE crescent_data SET USING n_clusters = 2 ---###\nk_means = KMeans(n_clusters = 2).fit(???)\n###\n\nfor mean in k_means.cluster_centers_:\n    graph.plot(mean[0], mean[1], 'ko', marker = '+', markersize = 20)\ngraph.scatter(crescent_data.T[0], crescent_data.T[1], c = k_means.labels_)\ngraph.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Again, a similar issue as with the circle data.\n\nBut k-means is just one method for clustering, other methods don't have quite the same restrictions as k-means.\n\nStep 6\n------\n\nSpectral clustering is a clustering method that aims to cluster data that is in some way connected - but not necessarily distributed."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn import cluster\n\n###--- REPLACE THE ??? BELOW WITH SpectralClustering ---###\nspectral = cluster.SpectralClustering(n_clusters = 2, eigen_solver = 'arpack', affinity = 'nearest_neighbors')\n###\n\n###--- REPLACE THE ??? BELOW WITH crescent_data ---###\nlabels_ = spectral.fit_predict(crescent_data)\n###\n\n### REPLACE THE ??? BELOW WITH labels_ ---###\ngraph.scatter(crescent_data.T[0], crescent_data.T[1], c = labels_)\n###\ngraph.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Let's use spectral clustering on the ring_data\n\n###--- REPLACE THE ??? BELOW WITH SpectralClustering ---###\nspectral = cluster.???(n_clusters = 2, eigen_solver = 'arpack', affinity = 'nearest_neighbors')\n###\n\n###--- REPLACE THE ??? BELOW WITH ring_data ---###\nlabels_ = spectral.fit_predict(????)\n###\n\n### REPLACE THE ??? BELOW WITH labels_ ---###\ngraph.scatter(ring_data.T[0], ring_data.T[1], c = ???)\n###\ngraph.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Does it classify the data in the correct clusters?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Conclusion\n\nWe have learnt two important clustering methods, k-means and spectral clustering, and used them on a variety of datasets where one might be more appropriate to use than the other."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}